#Writeup
---

###Camera Calibration

####1. Briefly state how you computed the camera matrix and distortion coefficients. Provide an example of a distortion corrected calibration image.

I reuse some code from the literature to calibrate the camera. First I use glob library to read in all the images. Then for each image, I call cv2.findChessboardCorners to recognize the corners of the chess boards. For each corner, it is put into the imgpoints and objpoints arrays. imgpoints contains the corners in the images and objpoints contains the objective "perfect" corner generated by np.mgrid.

The final step is to call cv2.calibrateCamera and pass in the two arrays to calculate mtx and dist. I store the values into a file by using pickle library, so that the data can be read out in the future.

For an example, please check my jupyter notebook under section 1.


###Pipeline (single images)

####1. Provide an example of a distortion-corrected image.

In the previous step I calibrated the camera and stored the parameters in a pickle file. 

In this step, I load the pickle file and call cv2.undistort to undistort images. Please find the output example from the section 2 in my jupyter notebook.

####2. Describe how (and identify where in your code) you used color transforms, gradients or other methods to create a thresholded binary image.  Provide an example of a binary image result.

The code is under section 2 in my jupyter notebook. Please find the pipeline functions to see the detail code.

Basically I use the combination of gradient methods of the gray figure, together with saturation to create the final binary. I first convert image to gray and calculate sobelx, sobely, magnitude of both gradient, and gradient direction. The binary pixel is 1 if:
gradx & grady =1 or magnitue & direction =1. 

I found the result cannot identify yello lanes very well, so I tried R channel and S channel. The problem of R channel is that some white-ish road and the sky also contain a lot of red color. S channel works better for identifying the yellow lane. So I eventually decide to use S channel as another parameter.

Please find the output images under Section 2 in my jupyter notebook for example.


####3. Describe how (and identify where in your code) you performed a perspective transform and provide an example of a transformed image.

The points I use are:
src:  [580, 460],[200, 720],[700, 460],[1140, 720]
dst:  [200, 100],[200, 720],[1040, 100],[1040, 720]
I use cv2.getPerspectiveTransform and cv2.warpPerspective functions to transform the original image to the new one. 

Please find the example images under section 3 in my jupyter notebook.

####4. Describe how (and identify where in your code) you identified lane-line pixels and fit their positions with a polynomial?

This part I largely reuse some codes from the literature.

From the birdeye binary image, the first step is to use:
histogram = np.sum(binary_warped[(int)(binary_warped.shape[0]/2):,:], axis=0)
to get the pixel histogram from the botom half of the image. Then I use two ways to find the fitted curve for both left and right lanes. The first is the sliding window method. Please find the detail code in my jupyter notebook under Section 4.

Once we get the initial curve, we can use a margin search method (my jupyter notebook under Section5). This method is to refer to previous calculated curve and only search pixels around a margin around the previous curve.

One technique I use is to refer to the history curves to calculate the current one. I calculated the recent history curve values and weighted average them to the current curve. 

The rationale is that the curve should not change too much from one frame to another. This technique makes the curve prediction more robust. 

Please find example output images under Section 5 and 6 in my jupyter notebook.


####5. Describe how (and identify where in your code) you calculated the radius of curvature of the lane and the position of the vehicle with respect to center.

Section 6 in my Jupyter notebook calculates the radius. First I get the fitted curve from the previous calculation. These curve is based on pixels. Then I use four random points on the curve and multiply them with xm_per_pix or ym_per_pix to convert to real world metric. Then I calculate a fitter curve again by using np.polyfit. Now the curve will be in real world metric.

The position of the car is determinied by the middle of the two lanes and the middle of the image. I just calculate the difference between this two values after converting them to real world metric.

Then I print the value on the image.

####6. Provide an example image of your result plotted back down onto the road such that the lane area is identified clearly.

I plot the green colored area back to the original undistored image. Please refer to the output image in section 7 in my jupyter notebook.


###Pipeline (video)

In section 9 of my jupyter notebook please find my final pipeline. I bookkept the past 5 frames' curve and radius in the pipelin so that I can refer to these value when I calculate the current frame. This technique makes the lane recognition more robust.

The final video is in the same git repo with name "final.mp4".

###Discussion

The major problem I encountered is to recognize the lanes by using saturation and gradient. My result of recognizing lanes is still not perfect. There are marks on the road and tree shadows that could be reognized as lanes. I wonder what is the proved best/optimal way to recognize the lanes.
